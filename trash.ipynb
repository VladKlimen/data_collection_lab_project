{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df42a44-d16a-4c55-adbe-6fc247fd198a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-504828327775848>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0;34m@\u001B[0m\u001B[0mudf\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mDoubleType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcosine_similarity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'DoubleType' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-504828327775848>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;34m@\u001B[0m\u001B[0mudf\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mDoubleType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcosine_similarity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'DoubleType' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'DoubleType' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "@udf (DoubleType())\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(v1.dot(v2) / ((v1.norm(2) * v2.norm(2))))\n",
    "\n",
    "\n",
    "@udf (DoubleType())\n",
    "def euqlidean_dist(v1, v2):\n",
    "    return 1 - float(v1.squared_distance(v2) ** 0.5)\n",
    "\n",
    "\n",
    "def get_similarities(embeddings_df, metric=cosine_similarity, cache=True, n_partitions=100):\n",
    "    embeddings_df1 = embeddings_df.withColumnRenamed(\"index\", \"index1\").withColumnRenamed(\"embedding\", \"embedding1\")                     \n",
    "    embeddings_df2 = embeddings_df.withColumnRenamed(\"index\", \"index2\").withColumnRenamed(\"embedding\", \"embedding2\")\n",
    "    sim_df = embeddings_df1.join(embeddings_df2, embeddings_df1[\"index1\"] < embeddings_df2[\"index2\"])\n",
    "\n",
    "    if cache:\n",
    "        sim_df.persist()\n",
    "    \n",
    "    sim_df = sim_df.repartition(n_partitions) \\\n",
    "                            .withColumn(\"similarity\", metric(F.col(\"embedding1\"), F.col(\"embedding2\"))) \\\n",
    "                            .select(\"index1\", \"index2\", \"similarity\")\n",
    "    \n",
    "    print(\"Calculating similarities...\")\n",
    "    sim_data = sim_df.collect()\n",
    "\n",
    "    return sim_data\n",
    "\n",
    "\n",
    "def similarities_to_dict(sim_data):  \n",
    "    col1, col2 = \"index1\", \"index2\"\n",
    "    sim_dict = {}\n",
    "\n",
    "    for row in tqdm(sim_data):\n",
    "        if row[col1] not in sim_dict:\n",
    "            sim_dict[row[col1]] = {}\n",
    "        sim_dict[row[col1]][row[col2]] = row[\"similarity\"]\n",
    "        if row[col2] not in sim_dict:\n",
    "            sim_dict[row[col2]] = {}\n",
    "        sim_dict[row[col2]][row[col1]] = row[\"similarity\"]\n",
    "    \n",
    "    return sim_dict\n",
    "\n",
    "\n",
    "def recalculate_mean_sim(elems_sim_dict, old_mean_dist, elems, new_elem):\n",
    "    total = len(elems) * (len(elems) + 1) * 0.5 - len(elems)\n",
    "    new_total = (len(elems) + 1) * (len(elems) + 2) * 0.5 - (len(elems) + 1)\n",
    "    new_mean_dist = old_mean_dist * total\n",
    "    for old_elem in elems:\n",
    "        try:\n",
    "            new_mean_dist += elems_sim_dict[new_elem][old_elem]\n",
    "        except Exception as e:\n",
    "            new_mean_dist += elems_sim_dict[old_elem][new_elem]\n",
    "    new_mean_dist /= new_total\n",
    "    return new_mean_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e1487a-97b9-442e-97ba-6541dedb6f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# some indian-style code... but it (mostly) works\n",
    "def cluster_sentences(sent_to_ind_map, sim_dict, min_sim=0.85, min_sim_threshold=0.8, min_sim_dec_step=0.01,\n",
    "                      min_mean_sim=0.75, min_mean_sim_threshold=0.7, min_mean_sim_dec_step=0.01):\n",
    "    indexes_dict = {index: {\"cluster\": None} for index in sent_to_ind_map}\n",
    "    print(f\"Sentences to cluster: {len(indexes_dict)}\")\n",
    "    indexes_iter = iter(indexes_dict)\n",
    "\n",
    "    clusters = {}\n",
    "    noise = {}\n",
    "    cluster_num = 0\n",
    "    get_next = True\n",
    "    iters = 0\n",
    "    clustered = 0\n",
    "\n",
    "    prev_epoch_clustered = 0\n",
    "    start = time.time()\n",
    "    while clustered < len(indexes_dict):  \n",
    "        try:\n",
    "            if get_next:\n",
    "                current_index = next(indexes_iter)\n",
    "            if indexes_dict[current_index][\"cluster\"] is not None:\n",
    "                get_next = True\n",
    "                continue\n",
    "            \n",
    "            sorted_sim = sorted(((value, key) for key ,value in sim_dict[current_index].items() \n",
    "                                 if indexes_dict[key][\"cluster\"] is None), reverse=True)\n",
    "            if len(sorted_sim) == 0:\n",
    "                get_next = True\n",
    "                continue\n",
    "\n",
    "            max_current_sim, closest_to_current = sorted_sim[0]\n",
    "            # Case: no unclustered sentence close enough to the current sentence\n",
    "            if max_current_sim < min_sim:\n",
    "                get_next = True\n",
    "                sorted_sim_clustered = sorted(((value, key) for key ,value in sim_dict[current_index].items() \n",
    "                                    if indexes_dict[key][\"cluster\"] is not None and indexes_dict[key][\"cluster\"] != -1), reverse=True)\n",
    "                \n",
    "                # Subcase: but there is a clustered sentence close enough to the current\n",
    "                if len(sorted_sim) > 0:\n",
    "                    new_mean_sim = 1\n",
    "                    i = 0\n",
    "\n",
    "                    # Find cluster to which the current sentence is similar enough\n",
    "                    while new_mean_sim >= min_mean_sim and i < len(sorted_sim_clustered):\n",
    "                        max_current_sim, closest_to_current = sorted_sim_clustered[i]\n",
    "                        i += 1\n",
    "                        # Oops, no sentence similar enough\n",
    "                        if max_current_sim < min_sim:\n",
    "                            break\n",
    "                        \n",
    "                        closest_cluster_num = indexes_dict[closest_to_current][\"cluster\"]\n",
    "                        new_mean_sim = recalculate_mean_sim(sim_dict, clusters[closest_cluster_num][\"mean_dist\"], \n",
    "                                                        clusters[closest_cluster_num][\"indexes\"], current_index)\n",
    "                        # Subcase: found cluster close eoungh to the sentence\n",
    "                        if new_mean_sim >= min_mean_sim:\n",
    "                            clustered += 1\n",
    "                            clusters[closest_cluster_num][\"mean_dist\"] = new_mean_sim\n",
    "                            clusters[closest_cluster_num][\"indexes\"].append(current_index)\n",
    "                            indexes_dict[current_index][\"cluster\"] = indexes_dict[closest_to_current][\"cluster\"]\n",
    "                            break\n",
    "\n",
    "            # Case: there is a sentence that close enough to the current sentence    \n",
    "            else:\n",
    "                # Subcase: new cluster\n",
    "                if cluster_num not in clusters:\n",
    "                    clustered += 2\n",
    "                    clusters[cluster_num] = {\"mean_dist\": max_current_sim, \"indexes\": [current_index, closest_to_current]}\n",
    "                    indexes_dict[current_index][\"cluster\"] = cluster_num\n",
    "                    indexes_dict[closest_to_current][\"cluster\"] = cluster_num\n",
    "                    current_index = closest_to_current\n",
    "                    get_next = False\n",
    "                \n",
    "                # Subcase: existing cluster, the current sentence is in it, trying to find the closest sentence to both the current sentence\n",
    "                # and the cluster\n",
    "                else: \n",
    "                    new_mean_sim = 1\n",
    "                    i = 1\n",
    "                    while new_mean_sim >= min_mean_sim and i < len(sorted_sim):\n",
    "                        new_mean_sim = recalculate_mean_sim(sim_dict, clusters[cluster_num][\"mean_dist\"], \n",
    "                                                        clusters[cluster_num][\"indexes\"], closest_to_current)\n",
    "                        # The closest sentence found\n",
    "                        if new_mean_sim >= min_mean_sim:\n",
    "                            clustered += 1\n",
    "                            clusters[cluster_num][\"mean_dist\"] = new_mean_sim\n",
    "                            clusters[cluster_num][\"indexes\"].append(closest_to_current)\n",
    "                            indexes_dict[closest_to_current][\"cluster\"] = cluster_num\n",
    "                            current_index = closest_to_current\n",
    "                            get_next = False\n",
    "                            break\n",
    "                        \n",
    "                        # Still not found, continue searching\n",
    "                        max_current_sim, closest_to_current = sorted_sim[i]\n",
    "                        i += 1\n",
    "                        if max_current_sim < min_sim:\n",
    "                            break\n",
    "                    \n",
    "                    # Not found sentence close enough both to the current sentence and it's cluster, the current cluster is formed,\n",
    "                    # create a new cluster\n",
    "                    if indexes_dict[closest_to_current][\"cluster\"] is None:\n",
    "                        get_next = True\n",
    "                        cluster_num += 1\n",
    "\n",
    "        except StopIteration:\n",
    "            iters += 1\n",
    "            if time.time() - start > 5:\n",
    "                print(f'''Iterations: {iters}''', end=('\\r'))\n",
    "                start = time.time()\n",
    "            \n",
    "            if clustered == prev_epoch_clustered:\n",
    "                min_mean_sim -= min_mean_sim_dec_step\n",
    "            stop = False\n",
    "            if round(min_mean_sim, 2) < min_mean_sim_threshold:\n",
    "                min_sim -= min_sim_dec_step\n",
    "                if round(min_sim, 2) < min_sim_threshold:\n",
    "                    for key in {key: value for key, value in indexes_dict.items() if value[\"cluster\"] is None}:\n",
    "                        clustered += 1\n",
    "                        indexes_dict[key][\"cluster\"] = -1\n",
    "                        if -1 not in clusters:\n",
    "                            clusters[-1] = {\"mean_dist\": 0, \"indexes\": [key]}\n",
    "                        else:\n",
    "                            clusters[-1][\"indexes\"].append(key)\n",
    "                    stop = True\n",
    "            if stop:\n",
    "                break\n",
    "            else:\n",
    "                prev_epoch_clustered = clustered\n",
    "                indexes_iter = iter(sim_dict)\n",
    "\n",
    "    print(f'''Iterations: {iters}''', end=('\\r'))\n",
    "    print(f'''\\nClustered: {clustered}, Noise: {len(clusters[-1][\"indexes\"]) \n",
    "          if -1 in clusters else 0}, Total clusters: {len(clusters)}''')\n",
    "    # print(\"\\nFinished\")\n",
    "    return clusters, indexes_dict, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fdad2a5-0a0e-4331-b4e0-ac90934f2dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-504828327775850>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m def get_similarities_dict(embeddings_dict, metric=cosine_similarity, cache=False, save_pickle=True, from_pickle=True, pickle_folder=\"\", \n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m                       pickle_path=\"/Workspace/Users/vladklim@campus.technion.ac.il/Project/similarities_dicts/\"):\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msave_pickle\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickle_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_folder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmakedirs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickle_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_folder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'cosine_similarity' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-504828327775850>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m def get_similarities_dict(embeddings_dict, metric=cosine_similarity, cache=False, save_pickle=True, from_pickle=True, pickle_folder=\"\", \n\u001B[0m\u001B[1;32m      2\u001B[0m                       pickle_path=\"/Workspace/Users/vladklim@campus.technion.ac.il/Project/similarities_dicts/\"):\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msave_pickle\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickle_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_folder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmakedirs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickle_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_folder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'cosine_similarity' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'cosine_similarity' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_similarities_dict(embeddings_dict, metric=cosine_similarity, cache=False, save_pickle=True, from_pickle=True, pickle_folder=\"\", \n",
    "                      pickle_path=\"/Workspace/Users/vladklim@campus.technion.ac.il/Project/similarities_dicts/\"):\n",
    "    if save_pickle and not os.path.exists(os.path.join(pickle_path, pickle_folder)):\n",
    "        os.makedirs(os.path.join(pickle_path, pickle_folder))\n",
    "\n",
    "    similarities_dict = {}\n",
    "    for i, category in enumerate(sorted(list(embeddings_dict.keys()))):\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        calculated = False\n",
    "        success = False\n",
    "        if from_pickle:\n",
    "            try:\n",
    "                with open(os.path.join(pickle_path, pickle_folder, \n",
    "                                       f'''similarities_{category.lower().replace(\" \", \"_\")}.pickle'''), \"rb\") as f:\n",
    "                    similarities_dict[category] = pickle.load(f)\n",
    "                print(f\"{category} - succsessfully loaded from pickle.\")\n",
    "                success = True\n",
    "            except:\n",
    "                print(f\"{category} - failed to load from pickle, performing calcuclations...\")\n",
    "                \n",
    "        if not success:\n",
    "            try:\n",
    "                similarities_data = get_similarities(embeddings_dict[category], metric=metric, cache=cache)\n",
    "                similarities_dict[category] = similarities_to_dict(similarities_data)\n",
    "                print(f\"{category} - successfully calculated similarities.\")\n",
    "                calculated = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"{category} - failed to calculate, skipping...\")\n",
    "                break\n",
    "\n",
    "        if calculated and save_pickle:\n",
    "            try:\n",
    "                with open(os.path.join(pickle_path, pickle_folder, \n",
    "                                       f'''similarities_{category.lower().replace(\" \", \"_\")}.pickle'''), \"wb\") as f:\n",
    "                    pickle.dump(similarities_dict, f)\n",
    "                print(f\"{category} - successfully saved to pickle.\")\n",
    "            except:\n",
    "                print(f\"{category} - failed to save to pickle, try to save from output.\")\n",
    "    print(\"\\nFinished.\")\n",
    "\n",
    "    return similarities_dict\n",
    "\n",
    "\n",
    "def get_clusters_dict(sent_to_ind_map, similarities_dict, save_pickle=True, from_pickle=True, \n",
    "                      min_sim=0.85, min_sim_threshold=0.8, min_sim_dec_step=0.01,\n",
    "                      min_mean_sim=0.75, min_mean_sim_threshold=0.7, min_mean_sim_dec_step=0.01,\n",
    "                      pickle_name=\"clusters_dict.pickle\", \n",
    "                      pickle_path=\"/Workspace/Users/vladklim@campus.technion.ac.il/Project/clusters_dicts/\"):\n",
    "    if save_pickle and not os.path.exists(pickle_path):\n",
    "        os.makedirs(pickle_path)\n",
    "\n",
    "    clusters_dict = {}\n",
    "    if from_pickle:\n",
    "        try:\n",
    "            with open(os.path.join(pickle_path, pickle_name), \"rb\") as f:\n",
    "                clusters_dict = pickle.load(f)\n",
    "            print(f\"Succsessfully loaded from pickle.\")\n",
    "            return clusters_dict\n",
    "        except:\n",
    "            print(f\"Failed to load from pickle, performing calcuclations...\")\n",
    "\n",
    "    print(\"Clustering started...\")\n",
    "    for category in sorted(list(similarities_dict.keys())):\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        clusters_dict[category] = cluster_sentences(sent_to_ind_map[category], similarities_dict[category],\n",
    "                    min_sim=min_sim, min_sim_threshold=min_sim_threshold, min_sim_dec_step=min_sim_dec_step,\n",
    "                    min_mean_sim=min_mean_sim, min_mean_sim_threshold=min_mean_sim_threshold, min_mean_sim_dec_step=min_mean_sim_dec_step)\n",
    "    \n",
    "    if save_pickle:\n",
    "        try:\n",
    "            with open(os.path.join(pickle_path, pickle_name), \"wb\") as f:\n",
    "                    pickle.dump(clusters_dict, f)\n",
    "            print(f\"Successfully saved to pickle.\")\n",
    "        except:\n",
    "            print(f\"Failed to save to pickle, try to save from output.\")\n",
    "\n",
    "    print(\"\\nFinished\")\n",
    "    \n",
    "    return clusters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829d6d92-0d1b-43c3-99ad-830f358be18b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf (VectorUDT())\n",
    "def elementwise_avg(vectors):\n",
    "    num_vectors = len(vectors)\n",
    "    vector_size = len(vectors[0])\n",
    "    avg_values = [0.0] * vector_size\n",
    "\n",
    "    for vector in vectors:\n",
    "        for i in range(vector_size):\n",
    "            avg_values[i] += vector[i] / num_vectors\n",
    "\n",
    "    return Vectors.dense(avg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f10ff3c0-8cc9-4622-90fb-10dd1d75de4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categories = [\"Automation Machinery Manufacturing\"]\n",
    "titles_embeddings_dict_trunc = {key: value for key, value in titles_embeddings_dict.items() if key in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a89d1720-9718-48e4-8200-bbef7e4732a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titles_similarities_dict = get_similarities_dict(titles_embeddings_dict_trunc, pickle_folder=\"titles\", save_pickle=True, from_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac3de99-1cca-4864-8d48-12d0f7392ab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titles_clusters_dict = get_clusters_dict(job_titles_toi, titles_similarities_dict, pickle_name=\"titles_clusters_dict.pickle\", \n",
    "                                        min_sim=0.85, min_sim_threshold=0.75, min_sim_dec_step=0.01,\n",
    "                                        min_mean_sim=0.8, min_mean_sim_threshold=0.7, min_mean_sim_dec_step=0.01, \n",
    "                                        save_pickle=True, from_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3703ebf-300c-4d39-abf5-df3ff43f3a81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "category = \"Automation Machinery Manufacturing\"\n",
    "indexes_clusters = titles_clusters_dict[category][1]\n",
    "get_cluster_udf = udf(lambda index: indexes_clusters[index]['cluster'], IntegerType())\n",
    "titles_embeddings_dict[category] = titles_embeddings_dict[category] \\\n",
    "                                        .withColumn(\"cluster\", get_cluster_udf(F.col(\"index\"))).withColumn(\"category\", F.lit(category))\n",
    "\n",
    "centroids_df = titles_embeddings_dict[category].groupBy(\"cluster\", \"category\") \\\n",
    "                                            .agg(elementwise_avg(F.collect_list(\"embedding\")).alias(\"centroid\"))\n",
    "\n",
    "titles_embeddings_dict[category] = titles_embeddings_dict[category].join(centroids_df, ['cluster', 'category'], 'left_outer') \\\n",
    "                                        .withColumn(\"sim_to_centroid\", cosine_similarity(F.col(\"embedding\"), F.col(\"centroid\"))) \\\n",
    "                                        .select(\"index\", \"sentence\", \"cluster\", \"sim_to_centroid\", \"category\", \"embedding\", \"centroid\")\n",
    "titles_embeddings_dict[category].persist()\n",
    "titles_embeddings_dict[category].display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ed6a97-b834-44b6-b359-ecf909e2bf00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_jobs_hist_by_industry():    \n",
    "    industries = tech_comps.select(\"industries\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    fig, axs = plt.subplots(len(industries)+1, figsize=(10, 25))\n",
    "\n",
    "    df = tech_comps.withColumn(\"jobs_number\", F.size(F.col(\"jobs_parsed\")))\n",
    "    axs[0].hist(df.select(\"jobs_number\").toPandas()[\"jobs_number\"], bins=20, alpha=0.7)\n",
    "    axs[0].set_title(\"All\")\n",
    "\n",
    "    for i, industry in enumerate(industries):\n",
    "        data = df.filter(df[\"industries\"] == industry).select(\"jobs_number\").toPandas()\n",
    "        axs[i+1].hist(data[\"jobs_number\"], bins=50, alpha=0.7)\n",
    "        axs[i+1].set_title(f\"{industry}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_jobs_hist_by_industry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea34faa-59ca-4e80-be32-37802d73ad4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75aab354-41c4-4840-b80c-5775523aa827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_titles_data = {}\n",
    "job_titles_toi = {}\n",
    "for industry in set(comps_industries.values()):\n",
    "    jobs_titles_data[industry] = sum([[(url, i, f'{url}_{i}', job[\"title\"]) for i, job in enumerate(value_dict[\"jobs\"])] \n",
    "                                 for url, value_dict in jobs_dict.items() if value_dict[\"industry\"] == industry], [])\n",
    "    job_titles_toi[industry] = {i: title for i, title in enumerate (list(set(sum([[job[\"title\"] for job in value_dict[\"jobs\"]] \n",
    "                                        for value_dict in jobs_dict.values() if value_dict[\"industry\"] == industry], []))))}\n",
    "sum([len(job_titles_toi[industry]) for industry in job_titles_toi.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db6c2677-b2f7-417a-8d36-8759301e21a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def optics_clustering(embeddings_df, \n",
    "                    metric='cosine', min_samples=3, pca=False, pca_k=50, max_eps=np.inf, xi=0.05, cluster_method=\"xi\"):\n",
    "    df, embedding_col = (embeddings_df, \"embedding\") if not pca else (get_PCA_components(embeddings_df, k=pca_k), \"embedding_pca\")\n",
    "    indexed_embeddings = {row[embedding_col]: row[\"index\"] for row in \n",
    "                          df.select(\"index\", embedding_col).collect()}\n",
    "    embeddings_list = list(indexed_embeddings.keys())\n",
    "\n",
    "    X = np.array(embeddings_list)\n",
    "    # clusters = OPTICS(min_samples=min_samples, metric=metric, \n",
    "    #                   n_jobs=-1, max_eps=max_eps, xi=xi, cluster_method=cluster_method).fit_predict(X)\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, cluster_selection_method='leaf',\n",
    "                            metric=metric, cluster_selection_epsilon=0.75, prediction_data=True).fit_predict(X)\n",
    "    \n",
    "    eval_score = metrics.silhouette_score(X, clusters, metric=metric)\n",
    "    print(f\"Silhouette score: {eval_score}\")\n",
    "    eval_score = metrics.calinski_harabasz_score(X, clusters)\n",
    "    print(f\"Calinski-Harabasz score: {eval_score}\")\n",
    "    eval_score = metrics.davies_bouldin_score(X, clusters)\n",
    "    print(f\"Davies-Bouldin score: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03a6a286-00d5-469a-9971-49e89874fa44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "centroids_df = titles_clusters.groupBy(\"cluster\", \"category\") \\\n",
    "                                            .agg(elementwise_avg(F.collect_list(\"embedding\")).alias(\"centroid\"))\n",
    "titles_clusters_centroids = titles_clusters.join(centroids_df, ['cluster', 'category'], 'left_outer') \\\n",
    "                                        .withColumn(\"euclidean_dist\", euqlidean_dist(F.col(\"embedding\"), F.col(\"centroid\"))) \\\n",
    "                                        .select(\"index\", \"sentence\", \"cluster\", \"soft_cluster\", \"euclidean_dist\", \"category\", \"embedding\", \"centroid\")\n",
    "titles_clusters_centroids.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "397604dc-bf33-4c3b-beeb-face27c66d17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Calculate cosine similarities\n",
    "df = titles_clusters_centroids.select(\"centroid\", \"cluster\").distinct().filter(F.col(\"cluster\") != F.lit(-1))\n",
    "df_cross = df.selectExpr(\"centroid as centroid1\", \"cluster as cluster1\") \\\n",
    "            .crossJoin(df.selectExpr(\"centroid as centroid2\", \"cluster as cluster2\"))\n",
    "df_similarity = df_cross.withColumn(\"euclidean_dist\", euqlidean_dist(col(\"centroid1\"), col(\"centroid2\")))\n",
    "\n",
    "# Find the most similar vector for each vector in the first DataFrame\n",
    "window = Window.partitionBy(\"centroid1\").orderBy(col(\"euclidean_dist\"))\n",
    "df_most_similar = df_similarity.withColumn(\"rn\", row_number().over(window)).filter(col(\"rn\") == 2).drop(\"rn\")\n",
    "df_most_similar.select(\"cluster1\", \"cluster2\", \"euclidean_dist\").orderBy(\"cluster1\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c26a9db6-ca62-48a8-8ec4-70785c550d92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pickle_path = \"/Workspace/Users/shalom2552@campus.technion.ac.il/project/students_skills_education_df.pickle\"\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    pickle_rdd = pickle.load(f)\n",
    "students_df = spark.createDataFrame(pickle_rdd)\n",
    "students_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0736fbec-5fbc-4704-9455-d68c9d4403e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This creates names \"layers\" for clusters. The method is simple: the most inner layer is the sentence itself.\n",
    "# The second layer is the max_ most frequent words in the sentences in each clusters. The third is the max_-1 most frequent, etc.\n",
    "# We filtered and preprocessed some of the words before layering, and also it is possible to start from n-th most frequent word, using start_from, \n",
    "# to obtain more meaningful names (sometimes it helps).\n",
    "def create_layers(clusters_df, min_=2, max_=5, min_word_len=2, start_from=0, lemmatize=True, soft=True):\n",
    "    df = clusters_df.withColumn(\"sentence_cleaned\", filter_nouns(F.col(\"sentence_cleaned\")))\n",
    "    cluster_col = \"soft_cluster\" if soft else \"cluster\"\n",
    "    current_sent_col = \"sentence_cleaned\"\n",
    "    current_cluster_col = cluster_col\n",
    "\n",
    "    for i in range(max_ - min_ + 1):\n",
    "        words = df.withColumn(\"word\", F.explode(F.split(F.col(current_sent_col), r\"\\s+|,\\s*\")))\n",
    "\n",
    "        if i == 0:\n",
    "            words = words.withColumn(\"word\", F.regexp_replace(\"word\", \"[^a-zA-Z+#/]\", \"\")) \\\n",
    "                            .filter(F.length(F.col(\"word\")) >= min_word_len)\n",
    "\n",
    "            lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "            @udf (StringType())\n",
    "            def lemmatize(word):\n",
    "                return lemmatizer.lemmatize(word)\n",
    "\n",
    "            if lemmatize: \n",
    "                words = words.withColumn(\"word\", lemmatize(\"word\"))\n",
    "                    \n",
    "        word_counts = words.groupBy(\"word\", current_cluster_col).count()\n",
    "\n",
    "        most_frequent_words = word_counts.orderBy(current_cluster_col, F.desc(\"count\"))\n",
    "\n",
    "        top_words_per_cluster = most_frequent_words.groupBy(current_cluster_col) \\\n",
    "            .agg(F.collect_list(\"word\").alias(\"words\")) \\\n",
    "            .select(current_cluster_col, F.when((F.size(F.col(\"words\")) >= max_ + start_from - i), \n",
    "                        F.concat_ws(\", \", get_first_n(F.col(\"words\"), F.lit(max_ - i), F.lit(start_from)))) \\\n",
    "            .otherwise(\"Unlabelled\" if i == 0 else F.col(current_sent_col)).alias(f\"layer_{i}\"))\n",
    "\n",
    "        if i == 0:\n",
    "            clusters_range = list(range(clusters_df.agg(F.min(cluster_col).alias(\"max_cluster\")).collect()[0][\"max_cluster\"],\n",
    "                            clusters_df.agg(F.max(cluster_col).alias(\"max_cluster\")).collect()[0][\"max_cluster\"] + 1))\n",
    "            remaining_clusters = [row[cluster_col] for row in top_words_per_cluster.select(cluster_col).distinct().collect()]\n",
    "            clusters_to_supplement = [c for c in clusters_range if c not in remaining_clusters]\n",
    "            if clusters_to_supplement != []:\n",
    "                top_words_per_cluster = top_words_per_cluster.union(\n",
    "                    spark.createDataFrame([(c, v) for c, v in zip(clusters_to_supplement, [\"Unlabelled\"] * len(clusters_to_supplement))], [cluster_col, f\"layer_{i}\"]))\n",
    "        \n",
    "        if i == 0:\n",
    "            df = top_words_per_cluster\n",
    "            df.persist()\n",
    "        else:\n",
    "            df = df.join(top_words_per_cluster, [current_cluster_col], \"left_outer\")\n",
    "\n",
    "        current_sent_col = current_cluster_col = f\"layer_{i}\"\n",
    "    \n",
    "    clusters_df = clusters_df.join(df, [cluster_col], \"left_outer\")\n",
    "\n",
    "    # for i in range(max_ - min_ + 1):\n",
    "    #     clusters_df = clusters_df.withColumn(f\"layer_{i}\", F.when(F.col(\"soft_probability\") < min_soft_prob, F.lit(\"Unlabelled\")) \\\n",
    "    #                                                         .otherwise(F.col(f\"layer_{i}\")))\n",
    "\n",
    "    return clusters_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259464c1-614f-4df6-a3b8-a7e5148961ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "02bb9c65-20f3-4997-b188-c382e6ad2d9a",
     "showTitle": true,
     "title": "Functions for sentence embeddings"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def remove_non_english_chars(text):\n",
    "        return re.sub(r'[^\\x00-\\x7F]+', '', text).strip()\n",
    "    \n",
    "\n",
    "# This removes the (...) parts from the given sentence (also [...] and {...}). For example, the \"(also [...] and {...})\"\" from the previous sentence will be removed.\n",
    "def remove_between_brackets(text):\n",
    "    pattern = r\"\\([^()]*\\)|\\[[^\\[\\]]*\\]|\\{[^{}]*\\}\"\n",
    "    result = re.sub(pattern, \"\", text)\n",
    "    return result\n",
    "\n",
    "\n",
    "# removes punctuation (replaces with spaces), removes repeating spaces\n",
    "def clean_text(text):\n",
    "    punctuation = '!\"$%&\\'()*-/:;<=>?[\\\\]^_`{|}~'\n",
    "    translator = str.maketrans(punctuation, ' ' * len(punctuation))\n",
    "    text = text.translate(translator)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_nouns_adjectives(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    filtered_words = [word for word, pos in tagged_words if pos.startswith('N') or pos.startswith('J')]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    if filtered_sentence == \"\":\n",
    "        return sentence\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def filter_nouns_adjectives_verbs(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    filtered_words = [word for word, pos in tagged_words if pos.startswith('N') or pos.startswith('J') or pos.startswith('V')]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    if filtered_sentence == \"\":\n",
    "        return sentence\n",
    "    return filtered_sentence\n",
    "    \n",
    "\n",
    "# The prep_pipeline is a list of functions from the above (just put the function in the list, as object)\n",
    "# The functions are applied on the sentence, by the order in the list (the order matters sometimes)\n",
    "def optional_preprocess(sentence, prep_pipeline, lower=True):\n",
    "    # example of prep_pipeline: [remove_non_english_chars, remove_between_brackets]\n",
    "    if lower:\n",
    "        prep_pipeline = [lambda s: s.lower()] + prep_pipeline\n",
    "    for func in prep_pipeline:\n",
    "        sentence = func(sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "    \n",
    "# Sentence embeddings\n",
    "def embed_sentences(data, pretrained_embeddings, prep_pipeline=[], \n",
    "                    remove_stopwords=True, clean=True, lower=True, normalize=False):\n",
    "    # Optional preprocessing / cleaning\n",
    "    optional_preprocess_udf = udf(lambda sentence: optional_preprocess(sentence, prep_pipeline, lower=lower), StringType())\n",
    "    data = data.withColumn(\"sentence_cleaned\", optional_preprocess_udf(F.col(\"sentence\")))\n",
    "    if clean:\n",
    "        data = data.withColumn(\"sentence_cleaned\", F.regexp_replace(F.col(\"sentence_cleaned\"), r\"[^a-zA-Z+#/-_ ]\", \"\"))\n",
    "\n",
    "    # Optional stop words removing. Some words are worth keeping in some cases. E.g \"it\" can stand for \"IT\" (Information Technologies), wchich is important in our case\n",
    "    keep_stopwords = [\"it\"]\n",
    "    if remove_stopwords:\n",
    "        english_stopwords = [word for word in stopwords.words('english') if word not in keep_stopwords]\n",
    "\n",
    "        tokenizer = ftr.Tokenizer(inputCol=\"sentence_cleaned\", outputCol=\"tokenized\")\n",
    "        data = tokenizer.transform(data)\n",
    "\n",
    "        remover = ftr.StopWordsRemover(inputCol=\"tokenized\", outputCol=\"filtered\", stopWords=english_stopwords)\n",
    "        data = remover.transform(data).withColumn(\"sentence_cleaned\", F.concat_ws(\" \", F.col(\"filtered\")))\n",
    "    \n",
    "    # The embedding part\n",
    "    documentAssembler = DocumentAssembler().setInputCol(\"sentence_cleaned\").setOutputCol(\"document\")\n",
    "    embeddings = pretrained_embeddings.setInputCols([\"document\"]).setOutputCol(\"sentence_embeddings\")\n",
    "    pipeline = Pipeline().setStages([\n",
    "        documentAssembler,\n",
    "        embeddings\n",
    "    ])\n",
    "\n",
    "    distinct_data = data.select(\"sentence_cleaned\").distinct()\n",
    "\n",
    "    result = pipeline.fit(distinct_data).transform(distinct_data)\n",
    "    result = result.select(\"sentence_cleaned\", \"sentence_embeddings\") \\\n",
    "            .withColumn(\"embedding\", F.expr(\"transform(sentence_embeddings, x -> x.embeddings)\")).drop(\"sentence_embeddings\")\n",
    "\n",
    "    # Optional normalization\n",
    "    if normalize:\n",
    "        normalizer = ftr.Normalizer(p=2.0).setInputCol(\"embedding_raw\").setOutputCol(\"embedding\")\n",
    "        result = normalizer.transform(result).select(\"sentence_cleaned\", \"embedding\")\n",
    "\n",
    "    # Convert the embedding to spark vector\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x[0]), VectorUDT())\n",
    "    result = result.select(\"sentence_cleaned\", \"embedding\").withColumn(\"embedding\", array_to_vector_udf(F.col(\"embedding\")))\n",
    "\n",
    "    embeddings_df = data.select(\"index\", \"sentence_cleaned\").join(result, \"sentence_cleaned\", \"inner\")\n",
    "\n",
    "    return embeddings_df\n",
    "\n",
    "\n",
    "# This function is relevant if you want to create a dict of embeddings by some category (in our case, industry was a category, but in the end we choose to not divide the data into categories. Yet, we used this function, with category \"all\", to not rewrite the code)\n",
    "def get_sentence_embeddings_dict(sent_dict, pretrained_embeddings, prep_pipeline=[], cache=True, n_partitions=100,\n",
    "                                 remove_stopwords=True, clean=True, lower=True, normalize=False):\n",
    "    sentence_embeddings_dict = {}\n",
    "\n",
    "    for category in sent_dict.keys():\n",
    "        data = sent_dict[category].select(\"index\", \"sentence\")\n",
    "        embeddings_df = embed_sentences(data, pretrained_embeddings, prep_pipeline=prep_pipeline,\n",
    "                                remove_stopwords=remove_stopwords, clean=clean, lower=lower, normalize=normalize)\n",
    "\n",
    "        sentence_embeddings_dict[category] = sent_dict[category] \\\n",
    "                .join(embeddings_df, \"index\", \"inner\") \\\n",
    "                .repartition(n_partitions)\n",
    "        if cache:\n",
    "            sentence_embeddings_dict[category].persist()\n",
    "\n",
    "    return sentence_embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cf9a9e9a-c840-46af-b491-e0a14fde666c",
     "showTitle": true,
     "title": "Functions for clustering"
    }
   },
   "outputs": [],
   "source": [
    "# returns pca components vector, given a column of vectors\n",
    "def get_PCA_components(embeddings_df, inputCol=\"embedding\", outputCol=\"embedding_pca\", k=50):\n",
    "    pca = PCA(k=k, inputCol=inputCol).setOutputCol(outputCol)\n",
    "    df_pca = pca.fit(embeddings_df).transform(embeddings_df)\n",
    "    return df_pca\n",
    "\n",
    "\n",
    "# This is a wrapper for hdbscan function (from the hdbscan package), works bad with the cosine similarity (at least in our case), so don't use the \"precomputed\" metric. Optionally, reduces the embeddings dimension using pca (we didn't use it in the final version)\n",
    "def hdbscan_clustering(embeddings_df, metric='precomputed', min_samples=2, min_cluster_size=5, cluster_selection_method='leaf',\n",
    "                           cluster_selection_epsilon=0.75, prediction_data=True, pca=False, pca_k=50):\n",
    "    if pca:\n",
    "        embedding_col = \"embedding_pca\"\n",
    "        pca_distinct = get_PCA_components(df.select(\"embedding\").distinct(), k=pca_k)\n",
    "        df = df.join(pca_distinct, \"embedding\", \"inner\")\n",
    "    else:\n",
    "        df, embedding_col = embeddings_df, \"embedding\"\n",
    "\n",
    "    # indexed_embeddings = list(zip(*[(row[embedding_col], row[\"index\"]) for row in \n",
    "    #                                     df.select(\"index\", embedding_col).collect()]))\n",
    "\n",
    "    embeddings_list = [row.embedding for row in df.select(\"embedding\").distinct().collect()]\n",
    "\n",
    "    X = np.array(embeddings_list)\n",
    "\n",
    "    if metric == \"precomputed\":\n",
    "        X = metrics.pairwise.cosine_similarity(X, X)\n",
    "        np.fill_diagonal(X, 0)\n",
    "    \n",
    "    # clusterer is our clustering model that we will use for cluster prediction of new points (aka empbedded sentences)\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, \n",
    "                                cluster_selection_method=cluster_selection_method, metric=metric, cluster_selection_epsilon=cluster_selection_epsilon, prediction_data=prediction_data,\n",
    "                                gen_min_span_tree=True).fit(X)\n",
    "    \n",
    "    clusters = clusterer.labels_\n",
    "    probs = clusterer.probabilities_\n",
    "    # the soft clusters are clusters which contain the original clustered points, and also points that are marked previously as noise. The noise points are appended to the clusters to which they are most probably belong (considering the existing clusters).\n",
    "    all_points_membership_vectors = hdbscan.all_points_membership_vectors(clusterer)\n",
    "    soft_clusters = [np.argmax(x) for x in all_points_membership_vectors]\n",
    "    soft_probs = [np.max(x) for x in all_points_membership_vectors]\n",
    "    \n",
    "    # These are different evaluation scores. The most relevant for hdbscan is DBCV, although it only helps to choose the right tunning, as it is not an absolute score and can't be used for comparing different clustering models on different kinds of data.\n",
    "    eval_score = metrics.silhouette_score(X, clusters, metric=metric)\n",
    "    print(f\"Silhouette score: {eval_score}\")\n",
    "    eval_score = metrics.calinski_harabasz_score(X, clusters)\n",
    "    print(f\"Calinski-Harabasz score: {eval_score}\")\n",
    "    eval_score = metrics.davies_bouldin_score(X, clusters)\n",
    "    print(f\"Davies-Bouldin score: {eval_score}\")\n",
    "    try:\n",
    "        eval_score = clusterer.relative_validity_\n",
    "        print(f\"DBCV relative score: {eval_score}\")\n",
    "    except Exception as e:\n",
    "        print(f\"DBCV:\\n{e}\")\n",
    "    print(f\"Number of clusters: {len(set(clusters)) - 1}\")\n",
    "    print(f\"Number of unique sentences clustered: {len([x for x in clusters if x != -1])}\")\n",
    "\n",
    "\n",
    "    # mapping clusters to sentences\n",
    "    clusters_map = [(embedding, int(cluster), int(soft_cluster), \n",
    "                     float(probability), float(soft_probability)) \n",
    "                    for i, (embedding, cluster, soft_cluster, probability, soft_probability) \n",
    "                    in enumerate(zip(embeddings_list, clusters, soft_clusters, probs, soft_probs))]\n",
    "    clusters_map_df = spark.createDataFrame(clusters_map, [\"embedding\", \"cluster\", \"soft_cluster\", \"probability\", \"soft_probability\"])\n",
    "\n",
    "    df_clustered = df.join(clusters_map_df, \"embedding\", \"inner\")\n",
    "\n",
    "    return df_clustered, clusterer\n",
    "\n",
    "\n",
    "# tsne embeddings are 2D vectors, obtained from embeddings of high dimension, used for visualisation of high dimensional vectors/points\n",
    "def get_tsne_embeddings(df_clustered, inputCol=\"embedding\", metric='cosine', \n",
    "                            random_state=None, early_exaggeration=12, perplexity=20):\n",
    "    df = df_clustered.select(inputCol).distinct()\n",
    "    df_tsne = df_clustered\n",
    "\n",
    "    # it's recommended to reduce the dimensionality to 50 with pca before applying tsne, to achieve better results\n",
    "    if len(df_clustered.first().asDict()[inputCol]) > 50:\n",
    "        df = get_PCA_components(df, inputCol=inputCol, k=50)\n",
    "        df_tsne = df_tsne.join(df, inputCol, \"inner\")\n",
    "        inputCol = \"embedding_pca\"\n",
    "        \n",
    "    vectors = df.select(inputCol).rdd.map(lambda x: tuple(x[inputCol].toArray())).collect()\n",
    "    vectors_tsne = TSNE(perplexity=perplexity, early_exaggeration=early_exaggeration,\n",
    "                   metric=metric, n_jobs=-1, random_state=random_state).fit_transform(vectors)\n",
    "    \n",
    "    # schema = StructType([StructField(inputCol, ArrayType(DoubleType(), True), True),\n",
    "    #                     StructField(\"vector_TSNE\", ArrayType(DoubleType(), True), True)])\n",
    "    vectors_df = spark.createDataFrame([Row(**{inputCol: Vectors.dense(v1), \"vector_TSNE\": Vectors.dense(v2.tolist())}) \n",
    "                                for v1, v2 in zip(vectors, vectors_tsne)], [inputCol, \"vector_TSNE\"])\n",
    "    df_tsne = df_tsne.join(vectors_df, inputCol, \"inner\").drop(\"embedding_pca\")\n",
    "\n",
    "    return df_tsne\n",
    "\n",
    "\n",
    "def generate_n_unique_colors(indexes, saturation=1, lightness=\"random\"):\n",
    "        saturation = random.uniform(0.9, 1) if saturation == \"random\" else saturation\n",
    "        lightness = random.uniform(0.2, 0.3) if lightness == \"random\" else lightness\n",
    "\n",
    "        colors = {}\n",
    "        for i in indexes:\n",
    "            r, g, b = colorsys.hls_to_rgb(random.uniform(0, 1), lightness, saturation)\n",
    "            r_hex = int(r * 255)\n",
    "            g_hex = int(g * 255)\n",
    "            b_hex = int(b * 255)\n",
    "            color_code = \"#{:02x}{:02x}{:02x}\".format(r_hex, g_hex, b_hex)\n",
    "            colors[i] = color_code\n",
    "        return colors\n",
    "\n",
    "\n",
    "# this function visualizes the clusters in 2D, but not as pretty as the datamapplot library we used for final visualization.\n",
    "# yet, we use it to analyze the clusters\n",
    "def visualize_tsne_clusters(vectors, clusters, probs, subtitle, soft_clusters=None, soft_probs=None, show_soft_clusters=True,\n",
    "                            show_noise=False, annotate=True, show_simplices=True, clusters_to_show=[], fig_size=(10, 10), s=200, fontsize=10, satur_accent=1, cent_prob=0.99):\n",
    "    x_coords, y_coords = zip(*vectors)\n",
    "    x_coords, y_coords = np.array(x_coords), np.array(y_coords)\n",
    "    centroids = {cluster: (np.mean(np.array([x for i, x in enumerate(x_coords) if clusters[i] == cluster and probs[i] > cent_prob])),\n",
    "                           np.mean(np.array([y for i, y in enumerate(y_coords) if clusters[i] == cluster and probs[i] > cent_prob])))\n",
    "                 for cluster in set(clusters)}\n",
    "    \n",
    "    if len(clusters_to_show) == 0:\n",
    "        clusters_to_show =  clusters\n",
    "    unique_clusters = list(set([x for x in clusters if x != -1]))\n",
    "    \n",
    "    # Calculate convex hulls for each cluster\n",
    "    hulls = {}\n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        cluster_points = np.array([vector for i, vector in enumerate(vectors)  if clusters[i] == cluster])\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = np.vstack([cluster_points[hull.vertices], cluster_points[hull.vertices[0]]])\n",
    "            hulls[i] = hull_points\n",
    "        except:\n",
    "             hulls[i] = None\n",
    "\n",
    "    # Interpolate hull points to create smooth area\n",
    "    smooth_hulls = {}\n",
    "    for i in range(len(unique_clusters)):\n",
    "        hull_points = hulls[i]\n",
    "        if hull_points is not None:\n",
    "            x, y = hull_points[:, 0], hull_points[:, 1]\n",
    "            t = np.linspace(0, 1, len(x))\n",
    "            interp = interp1d(t, np.vstack([x, y]), kind='cubic', axis=1)\n",
    "            t_smooth = np.linspace(0, 1, 100)\n",
    "            smooth_points = interp(t_smooth).T\n",
    "            smooth_hulls[i] = smooth_points\n",
    "        else:\n",
    "            smooth_hulls[i] = None\n",
    "\n",
    "    color_palette = generate_n_unique_colors(unique_clusters)\n",
    "    color_palette = {key: (value if key in clusters_to_show else 'white') for key, value in color_palette.items()}\n",
    "    color_palette[-1] = (0.5, 0.5, 0.5)\n",
    "    zorders = {key: (2 if key in clusters_to_show else 0) for key in clusters}\n",
    "    alphas = {\"cluster\": 0.1, \"soft_cluster\": 0.1, \"noise\": 0.03}\n",
    "\n",
    "    cluster_colors = [color_palette[x] for x in clusters]\n",
    "    cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, probs)]\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    if show_noise:\n",
    "        for i, (x, y, cluster) in enumerate(zip(x_coords, y_coords, clusters)):\n",
    "            if cluster == -1:\n",
    "                plt.scatter(x, y, color=cluster_member_colors[i], alpha=alphas[\"noise\"], s=s/2, zorder=1)\n",
    "\n",
    "    if show_soft_clusters and soft_clusters is not None and soft_probs is not None:\n",
    "        alphas[\"cluster\"] = alphas[\"soft_cluster\"] * 2\n",
    "        soft_cluster_colors = [sns.desaturate(color_palette[x], min(1, p * satur_accent)) for x, p in zip(soft_clusters, soft_probs)]\n",
    "        for i, (x, y, soft_cluster) in enumerate(zip(x_coords, y_coords, soft_clusters)):\n",
    "            if clusters[i] == -1 and soft_cluster in clusters_to_show:\n",
    "                plt.scatter(x, y, color=soft_cluster_colors[i], alpha=alphas[\"soft_cluster\"], s=s, zorder=3)\n",
    "                \n",
    "    for i, (x, y, cluster) in enumerate(zip(x_coords, y_coords, clusters)):\n",
    "        if cluster != -1:\n",
    "            plt.scatter(x, y, color=cluster_member_colors[i], alpha=alphas[\"cluster\"], s=s, zorder=zorders[cluster])\n",
    "    \n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        x, y = centroids[cluster]\n",
    "        smooth_points = smooth_hulls[i]\n",
    "        if show_simplices and smooth_points is not None:\n",
    "            plt.fill(smooth_points[:, 0], smooth_points[:, 1], color=color_palette[cluster], alpha=0.05, zorder=zorders[cluster])\n",
    "        if annotate:\n",
    "            plt.text(x, y, str(cluster), color=color_palette[cluster], ha='center', va='center', \n",
    "                     fontsize=fontsize, zorder=zorders[cluster])\n",
    "\n",
    "    plt.title(f\"2D Approximate Relative Visual Representation of Clusters ([0-{max(clusters)}])\\n({subtitle})\", \n",
    "              fontsize=fig_size[0] + 3)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2f22409c-2cb2-4f92-88cd-26a593c20afa",
     "showTitle": true,
     "title": "Additional UDF functions"
    }
   },
   "outputs": [],
   "source": [
    "@udf (VectorUDT())\n",
    "def elementwise_avg(vectors):\n",
    "    num_vectors = len(vectors)\n",
    "    vector_size = len(vectors[0])\n",
    "    avg_values = [0.0] * vector_size\n",
    "\n",
    "    for vector in vectors:\n",
    "        for i in range(vector_size):\n",
    "            avg_values[i] += vector[i] / num_vectors\n",
    "\n",
    "    return Vectors.dense(avg_values)\n",
    "\n",
    "\n",
    "@udf (DoubleType())\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(v1.dot(v2) / ((v1.norm(2) * v2.norm(2))))\n",
    "\n",
    "\n",
    "@udf (DoubleType())\n",
    "def euqlidean_dist(v1, v2):\n",
    "    return float(v1.squared_distance(v2) ** 0.5)\n",
    "\n",
    "\n",
    "@udf (ArrayType(StringType()))\n",
    "def get_first_n(arr, n, start_from):\n",
    "    return list(arr)[start_from : n + start_from]\n",
    "\n",
    "\n",
    "# This used for correcting the list wrapped in string, we got from Gemini as the answer to the promt\n",
    "@udf (ArrayType(StringType()))\n",
    "def string_to_list(list_string):\n",
    "    return [s.strip() for s in list_string[1:-1].split(r', ')]\n",
    "\n",
    "\n",
    "@udf (StringType())\n",
    "def filter_nouns(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    filtered_words = [word for word, pos in tagged_words if pos.startswith('N')]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    if filtered_sentence == \"\":\n",
    "        return sentence\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "188db24d-0eb8-4d34-a6a4-b905a77a7a62",
     "showTitle": true,
     "title": "utilites"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_pickle(data, file_name, dest_dir=\"/Workspace/Users/vladklim@campus.technion.ac.il/Project/models/\"):\n",
    "    Path(\"/Workspace/Users/vladklim@campus.technion.ac.il/Project/models/\").mkdir(parents=True, exist_ok=True)\n",
    "    file_path = os.path.join(dest_dir, file_name)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "# This used to split files larger than 10mb, because databricks didn't allow to download large files \n",
    "def split(source, dest_dir, files_name, write_size=10**7):\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "    else:\n",
    "        for file in os.listdir(dest_dir):\n",
    "            os.remove(os.path.join(dest_dir, file))\n",
    "    part_num = 0\n",
    "    \n",
    "    with open(source, 'rb') as input_file:\n",
    "        while True:\n",
    "            chunk = input_file.read(write_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            part_num += 1\n",
    "            file_path = os.path.join(dest_dir, files_name + str(part_num))\n",
    "            \n",
    "            with open(file_path, 'wb') as dest_file:\n",
    "                dest_file.write(chunk)\n",
    "    \n",
    "    print(f\"Partitions created: {part_num}\")\n",
    "\n",
    "\n",
    "# Used to join the splitted files back\n",
    "def join(source_dir, dest_file, read_size):\n",
    "    with open(dest_file, 'wb') as output_file:\n",
    "        for path in os.listdir(source_dir):\n",
    "            with open(path, 'rb') as input_file:\n",
    "                while True:\n",
    "                    bytes = input_file.read(read_size)\n",
    "                    if not bytes:\n",
    "                        break\n",
    "                output_file.write(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a3564c4-5945-412c-9343-3fde610434a4",
     "showTitle": true,
     "title": "Functions for creating naming layers of the clusters"
    }
   },
   "outputs": [],
   "source": [
    "# This creates names \"layers\" for clusters. The method is simple: the most inner layer is the sentence itself.\n",
    "# The second layer is the max_ most frequent words in the sentences in each clusters. The third is the max_-1 most frequent, etc.\n",
    "# We filtered and preprocessed some of the words before layering, and also it is possible to start from n-th most frequent word, using start_from, \n",
    "# to obtain more meaningful names (sometimes it helps).\n",
    "def create_layers(clusters_df, min_=2, max_=5, min_word_len=2, start_from=0, lemmatize=True, soft=True):\n",
    "    df = clusters_df.withColumn(\"sentence_cleaned\", filter_nouns(F.col(\"sentence_cleaned\")))\n",
    "    cluster_col = \"soft_cluster\" if soft else \"cluster\"\n",
    "    sent_col = \"sentence_cleaned\"\n",
    "\n",
    "    words = df.withColumn(\"word\", F.explode(F.split(F.col(sent_col), r\"\\s+|,\\s*\")))\n",
    "    words = words.withColumn(\"word\", F.regexp_replace(\"word\", \"[^a-zA-Z+#/]\", \"\")) \\\n",
    "                    .filter(F.length(F.col(\"word\")) >= min_word_len)\n",
    "\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    @udf (StringType())\n",
    "    def lemmatize(word):\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "    if lemmatize: \n",
    "        words = words.withColumn(\"word\", lemmatize(\"word\"))\n",
    "                    \n",
    "    word_counts = words.groupBy(\"word\", cluster_col).count()\n",
    "    most_frequent_words = word_counts.orderBy(cluster_col, F.desc(\"count\"))\n",
    "\n",
    "    top_words_per_cluster = most_frequent_words.groupBy(cluster_col) \\\n",
    "        .agg(F.collect_list(\"word\").alias(\"words\")) \\\n",
    "        .select(cluster_col, F.when(F.size(F.col(\"words\")) >= max_ + start_from, \n",
    "                    get_first_n(F.col(\"words\"), F.lit(max_), F.lit(start_from))) \\\n",
    "        .otherwise(F.array(F.lit(\"Unlabelled\"))).alias(f\"top_{max_}_words\"))\n",
    "\n",
    "    clusters_range = list(range(clusters_df.agg(F.min(cluster_col).alias(\"max_cluster\")).collect()[0][\"max_cluster\"],\n",
    "                    clusters_df.agg(F.max(cluster_col).alias(\"max_cluster\")).collect()[0][\"max_cluster\"] + 1))\n",
    "    remaining_clusters = [row[cluster_col] for row in top_words_per_cluster.select(cluster_col).distinct().collect()]\n",
    "    clusters_to_supplement = [c for c in clusters_range if c not in remaining_clusters]\n",
    "    if clusters_to_supplement != []:\n",
    "        top_words_per_cluster = top_words_per_cluster.union(\n",
    "            spark.createDataFrame([(c, v) for c, v in zip(clusters_to_supplement, [\"Unlabelled\"] * len(clusters_to_supplement))], [cluster_col, f\"top_{max_}_words\"]))\n",
    "        \n",
    "    df = top_words_per_cluster\n",
    "    df.persist()\n",
    "\n",
    "    for i in range(max_ - min_ + 1):\n",
    "        df = df.withColumn(f\"layer_{i}\", F.concat_ws(\", \", get_first_n(F.col(f\"top_{max_}_words\"), F.lit(max_ - i), F.lit(0))))\n",
    "    \n",
    "    clusters_df = clusters_df.join(df.drop(f\"top_{max_}_words\"), [cluster_col], \"left_outer\")\n",
    "    clusters_df.persist()\n",
    "\n",
    "    return clusters_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68d922f5-07cb-4dbe-b93c-e1ee7158b4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf (StringType())\n",
    "def extract_job_function(s):\n",
    "    result = re.findall(r\"Job function=(.*?), Seniority level\", s)\n",
    "    if len(result) > 0:\n",
    "        return result[0]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "@udf (ArrayType(StringType()))\n",
    "def split_job_functions(s):\n",
    "    result = re.split(r', and | and |, ', s)\n",
    "    if result == ['']:\n",
    "        return [\"Unlabelled\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b146286a-9138-4a15-a8c1-53b9a6ebb784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Predicting clusters for the remaining dataset with the trained clusterer<br>\n",
    " (this takes a lot of time... we didn't do it in the end, worked only with the 10000 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133ba687-3666-44ed-a01c-f4e14056cf46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skills_clusters_full = skills_embeddings_dict[\"all\"] \\\n",
    "                        .join(skills_clusters.drop(\"sentence\", \"sentence_cleaned\", \"embedding\"), \"index\", \"outer\") \\\n",
    "                        .fillna(-1, subset=[\"cluster\", \"soft_cluster\"]) \\\n",
    "                        .fillna(0, subset=[\"probability\", \"soft_probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152c1a93-09b2-436e-ac25-1e8a1b8875ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unclustered_skills_embeddings = skills_clusters_full.filter(F.col(\"soft_cluster\") == -1).select(\"index\", \"embedding\")\n",
    "indexed_skills_embeddings = {row[\"embedding\"]: row[\"index\"] for row in unclustered_skills_embeddings.collect()}\n",
    "skills_embeddings_list = list(indexed_skills_embeddings.keys())\n",
    "skills_to_predict = np.array(skills_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d830be-7cf2-48a7-a1e9-ffc4e88f5c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skills_soft_clusters = [np.argmax(x) for x in skills_membership_vectors]\n",
    "skills_soft_probs = [np.max(x) for x in skills_membership_vectors]\n",
    "skills_clusters_map = {skills_embeddings_list[i]: (int(soft_cluster), float(soft_probability)) \n",
    "                    for i, (soft_cluster, soft_probability) \n",
    "                    in enumerate(zip(skills_soft_clusters, skills_soft_probs))}\n",
    "get_soft_cluster_udf = F.udf(lambda embedding: skills_clusters_map[embedding][0] \n",
    "                             if embedding in skills_clusters_map else None, IntegerType())\n",
    "get_soft_prob_udf = F.udf(lambda embedding: skills_clusters_map[embedding][1] \n",
    "                          if embedding in skills_clusters_map else None, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d25d05-8ea2-46a3-9ae5-64b3cf75607e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skills_membership_vectors = hdbscan.prediction.membership_vector(skills_clusterer, skills_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b29bc3b-5c2e-4115-a208-4754e45a7aeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[82]: DataFrame[index: int, sentence: string, cluster: bigint, soft_cluster: bigint, probability: double, soft_probability: double]"
     ]
    }
   ],
   "source": [
    "skills_clusters_full_denoised = skills_clusters_full \\\n",
    "                    .withColumn(\"soft_cluster_temp\", get_soft_cluster_udf(F.col(\"embedding\"))) \\\n",
    "                    .withColumn(\"soft_prob_temp\", get_soft_prob_udf(F.col(\"embedding\"))) \\\n",
    "                    .withColumn(\"soft_cluster\", F.when(F.col(\"soft_cluster_temp\").isNull(), \n",
    "                                                       F.col(\"soft_cluster\")).otherwise(F.col(\"soft_cluster_temp\"))) \\\n",
    "                    .drop(\"soft_cluster_temp\") \\\n",
    "                    .withColumn(\"soft_probability\", F.when(F.col(\"soft_prob_temp\").isNull(), \n",
    "                                                           F.col(\"soft_probability\")).otherwise(F.col(\"soft_prob_temp\"))) \\\n",
    "                    .drop(\"soft_prob_temp\", \"embedding\", \"sentence_cleaned\")\n",
    "skills_clusters_full_denoised.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1aed8c8-c5de-414a-b03a-8d88af102b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skills_clusters_full_denoised_fix = skills_clusters_full_denoised \\\n",
    "    .join(skills_clusters_full.selectExpr(\"index\", \"soft_probability as soft_prob_initial\", \"embedding\", \"sentence_cleaned\"), \"index\", \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b12927-16fc-43ab-9248-1cdc7262ba05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skills_clusters_full_denoised_fix = skills_clusters_full_denoised_fix \\\n",
    "    .withColumn(\"probability\", F.when(F.col(\"soft_prob_initial\") == 0, \n",
    "                                F.col(\"soft_probability\")).otherwise(F.col(\"probability\"))) \\\n",
    "    .drop(\"soft_probability\").withColumnRenamed(\"soft_prob_initial\", \"soft_probability\") \\\n",
    "    .withColumn(\"soft_cluster\", F.when((F.col(\"soft_probability\") > 0) & (F.col(\"soft_probability\") < 0.004), \n",
    "                                F.lit(-1)).otherwise(F.col(\"soft_cluster\"))) \\\n",
    "    .withColumn(\"soft_cluster\", F.when((F.col(\"probability\") < 0.2), \n",
    "                                F.lit(-1)).otherwise(F.col(\"soft_cluster\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "413ea57b-0153-465d-9718-93264fa2d336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[210]: 75588"
     ]
    }
   ],
   "source": [
    "skills_clusters_full_denoised_fix.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "trash",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
